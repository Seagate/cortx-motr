#+TITLE: Distributed transaction manager high level design (draft)
#+AUTHOR: DTM Team, Anatoliy Bilenko
#+DATE: 28-12-2020
#+OPTIONS: toc:1


This document presents a high level design (HLD) of distributed
transaction management component for Motr. The main purposes of this
document are: (i) to be inspected by Motr, HA and HARE architects and peer
designers to ascertain that high level design is aligned with Motr, HA and
HARE architecture and other designs, and contains no defects, (ii) to
be a source of material for Active Reviews of Intermediate Design
(ARID) and detailed level design (DLD) of the same component, (iii) to
serve as a design reference document.

The intended audience of this document consists of Motr, HA and HARE
customers, architects, designers and developers.

* Revision history
|   | Version | Date             | Author   | Desciption of changes                     |
|---+---------+------------------+----------+-------------------------------------------|
|   |   0.0.1 | <2020-12-28 Mon> | Anatoliy | Initial                                   |
|   |   0.0.2 | <2020-12-29 Tue> | Anatoliy | Added significant sections                |
|   |   0.0.3 | <2020-12-29 Tue> | Anatoliy | Added usecases worked through by the team |
* Approvals
|   | Version | approving person | Signature, date |
|---+---------+------------------+-----------------|
|   |   0.0.1 |                  |                 |
* Introduction
#+BEGIN_QUOTE
=[This section succinctly introduces the subject matter of the design. 1--2 paragraphs]=
#+END_QUOTE

This document describes high level implementation details of metadata
DTM or DTM0 including protocol properties, interraction with related
components such as HA, DTM user (S3), constrains, assumptions and
usecases. DTM0 is intended to addresses scalability, durability,
availablility and other requirements of MRD and explains how to reach
them.

* Definitions
#+BEGIN_QUOTE
=[Definitions of terms and concepts used by the design go here. The definitions must be as precise as possible. References to the Mero Glossary are permitted and encouraged. Agreed upon terminology should be incorporated in the glossary]=
#+END_QUOTE

** process

Operating system process which is under the track of HA that can be
associated with defined set of process states.
** process state

State in sense of availability of an arbitrary software or hardware
component of motr treated by HA and SEEN on the process level. It's
assumed that process with the help of HA can distinguish ONLINE,
TRANSIENT, FAILED, REPAIRING states.
** motr process

Processes related to DTM0 algorithm.
** motr HA

High availability subsystem of motr responsible for orchestration of
motr processes. Orchestration itself is a process of communication
between HA and motr processes by sending corresponding messages and
signals (kill, start, restart). Processes involved into orchestration
and HA itself follow specific logic seen outside and which can be
tracked by a chain of processes state changes.
** motr service

A part of motr process responsible for sending, receiving and reaction
on HA and DTM messages along with other specific activities (IO
services, RM services, etc).
** message

motr RPC item (one-way or request-reply) sent from one process and
delivered to another via RPC protocol, having appropriate properties,
attributes and payloads.
** HA message

Different types of messages sent in between motr services and motr HA
accordingly to orchestration protocol of communication. Such messages,
sent from HA to process, carry information about process states
(process_x goes ONLINE) and messages-triggers, sent in bidirectional
manner w.r.t. to update process state at all ends of communication (HA
sends "start recovery" message, process sends "recovery complete"
message).
** DTM0

Distributed transaction manager is a component or an algorithm
maintaining consistency of storage replicas of *motr metadata* in
distributed system in face of failures simplifying handling such
failures by means of hiding failure processing logic and providing a
distributed transactional interface with a specific semantics to the
user.
** DTM0 message

Different types of messages sent in between DTM0 services accordingly
to DTM0 protocol of the communication. Payloads of these messages may
contain modification requests to persistent state stored on underlying
storage (PUT, DEL, REDO, etc.), service information (participants, dtx
id, version, etc.), acknowledgments messages (PERSISTENT, EXECUTED,
etc.).
** dtx

Distributed transaction is a unit of modification of the distributed
state (metadata replicas for DTM0) for the whole distributed system
performed and treated in a reliable and coherent manner complying with
the atomicity and durability properties (no isolation and consistency
for DTM0). Keeping track of dependencies is performed by the dtx usage
semantics on the originator side and will be covered later.

** dtx state

The state of the dtx is seen in many places in the system still
treated on the originator side. dtx itself limits states to EXECUTED,
PERSISTENT, STABLE, DONE which will be covered later.

** txr

Distributed transaction record is a payload of DTM0 message, delivered
onto participants of dtx with persistent state via DTM0 messages
onto DTM0 services by any possible means (inside CAS request data or
as a separate network message, etc.). It includes list of dtx
participants, information regarding underlying operation (PUT, DEL,
etc.) complete enough to execute this operation locally and without
any network communication with any other participants and to reach the
persistent effect.

** operation

Underlying operation for DTM0 is a request to motr metadata CAS
service to transactional update the underlying state of metadata
storage (PUT, DEL, etc.) provided by any possible means either by
sending messages over the network or by performing execution of local
to the process FOM.

Operation is being performed in the context of local transaction.
** update

Part of an operation that a remote Mero instance must execute is called an update.
** local transaction

a transaction is a collection of modifications of persistent state.
closed transaction is atomic with respect to process crashes that
happen after transaction close call returns.

** participant

a process, sending, receiving DTM0 messages and executing underlying
operations w.r.t. DTM0 algorithm. Participant may have persistent and
volatile state.
** originator

a process, a special kind of participant which initiates distributed
transaction and receives results of local data modifications (PUT,
DEL, etc) related to its context.
** recovery

a process of communication accordingly to DTM0 protocol between DTM0
services by means of DTM0 messages and execution of underlying
operations initiated by HA. The goal of recovery process is to restore
consistency of the distributed state across distributed system replicas.
** failure

Loss of persistent or/and volatile state of the process or its parts.
Seen on participant by receiving corresponding HA messages with the
process state changes. Treated and identified on HA side only
accrodingly to HA internal logic.
** transient failure

Failure recoverable with DTM0 (process crash, restart, etc.)
** permanent failure

Failure unrecoverable with DTM0 (storage controller failure, etc.)
** dependency

There's not explicit dependency treatment in DTM0 design, still such
dependencies can be injected on the originator side and treated on the
participant side by means of ordering dtxs in time. For example,
originator may wait until dtx1 gets to STABLE state before executing
dtx2.
** clock

an algorithm used to generate versions and tx ids in distributed system
having property of the order.
** version

a special number used for application of operations in specific order
supporting ordering operator.
** log

DTM0 log, persistent structure used by DTM0 algorithm to provide recovery.
** persistent state

Persistent data stored in underlying storage.
** volatile state

Volatile data stored in RAM.
** tombstone

is a special metadata record indicating that normal key and value
record were deleted some time ago and can be interpreted during DTM0
recovery and other modes.

* Requirements & assumptions
#+BEGIN_QUOTE
=[This section enumerates requirements collected and reviewed at the Requirements Analysis (RA) and Requirements Inspection (RI) phases of development. References to the appropriate RA and RI documents should go here. In addition this section lists architecture level requirements for the component from the Summary requirements table and appropriate architecture documentation]=
#+END_QUOTE
** [A.dtm0.ad-tx]
Transactions support atomicity and durability only.
** [A.dtm0.1-op]
DTM0 supports only one operation (PUT, DEL, etc) in a single dtx.
** [A.dtm0.txr]
A single record shall have enough data to restore consistency in all
replicas. Therefore dtx is fully described by a single txr. A txr
contains the list of participants and a participant can
deterministically execute its part of transaction based on txr without
communicating with other participants

** [A.dtm0.no-dep]
DTM0 doesn't have explicit dependency tracking and dependencies
between operations can be introduced by ordering their execution and
stabilisation with an appropriate DTX interface. Additionally to
mentioned above if it's needed users of DTM0 should track any other
dependency except order by themselves.
** [A.no-undo]
DTM0 algo doesn't support undo of the operation but may support it in
future.
** [A.dix.no-spares]
DIX doesn't use spares in the DTM0 algorithm may be extendend in the
future.
** [A.ha.failures]
Motr doesn't treat failures types and relies on HARE.
** [A.dtm0.failures]
DTM0 algorithm treats transient failures only
** [A.rpc.timeout]
Timeout is not a failure
** [A.dtm0.transient.failures]
DTM0 supports handling of not more that N+K-1 transient failures
during the period of interest. In cases when previous failures are
being repaired by DTM and new occurs, the previous one is treated as
transient.
** [A.rpc.magic-link]
RPC resends messages untill it get stopped by HA message. see
A.rpc.timeout.
** [A.dix.no-vectored]
Due to complexity of imask value distribution, DTM0 doesn't support
vectored dix operations transmitting several keys and values to the
counterpart. S3Server can be touched here.

** [A.clock.sync]
DTM0 relies on the synchronised 'physical' clock over all nodes and
not responsible to setting these clocks.
** [A.clock.desync]
In case of desyncronisation of clocks DTM0 shall not corrupt metadata
still not responsible for setting the clock.

** [A.dtx.cancel]
DTM0 doesn't support cancelation of DTXs.
** [A.HA.messages]
DTM0 introduces new HA related messages that need to be reacted
properly on HA side.
** [A.S3.integration]
DTM0 design is not responsible for the definition of flawless S3
integration procedure including IO data path failures which are not in
the scope of DTM0.
** [R.dtm0.log.payload]
DTM0 log may contain long-living data
** [R.dtm0.versioning]
Conflict resolution algorithm is based on top of versions numbers
which can be easily compared. A participant can deterministically tell
whether it already executed the dtx.
** [A.dtm0.fairness]
The period of failure occurence statistically is much longer than dtm0 recovery time.

** [A.originator.failure]
Originator failure is treated as a permanent failure

** [A.ha.failure-model]
Failure model is not defined by DTM0 design, still DTM0 code has to
distinguish at least the following states of the participants: ONLINE,
TRANSIENT, FAILED, RECOVERING.

** [A.ha.EOS]
HA provides exactly one semantics including and not excluding the
following usecase: before, after and during the time of transient
failure of the participant all HA messages regarding cluster state
changes delivered on other online participants shall be delivered to
the failing participant after it gets online in the same order they
were delivered to others.

* Design highlights
#+BEGIN_QUOTE
=[This section briefly summarises key design decisions that are important for understanding of the functional and logical specifications and enumerates topics that a reader is advised to pay special attention to]=
#+END_QUOTE

* Functional specification
#+BEGIN_QUOTE
=[This section defines a functional structure of the designed component: the decomposition showing *what* the component does to address the requirements]=
#+END_QUOTE

=[interface]= DTM0 component interracts with the user (clovis
interface user, s3 server) by means of the asynchronous dtx interface
integrated into clovis interface, interracts with other motr processes
or so-called participants of dtx by means of trasmitting [[*DTM0 messages][DTM0
messages]], interracts with HA by means of transmitting [[*HA messages][HA messages]].

=[input]= The user attaches txr to outgoing DTM0 message transmitted
to the participants. Internally delivery, reply and other state changes
related to DTM0 messages trigger dtx state change which might be
observed during interraction with [[*dtx interface][dtx interface]]. Error codes might be
returned to the user via dtx attributes.

=[output]= On the component level interraction can be seen as a flow
of HA and DTM0 messages and [[*Persistent state changes][persistent state changes]].

** dtx interface
Provides a way to *open* a [[*dtx][dtx]], *add* an [[*operation][operation]] (PUT, DEL, etc) to
dtx, to *commit* the dtx so that it goes into processing, to
*subscribe* or *wait* until dtx moves into specific state and to
*finalise* dtx.

The following states and their semantics can be relevant to dtx still
the list can be non-full:
 - *EXECUTED* one or more [[*operation][operations]] have been executed in volatile
   memory of the participant, the result of such execution is known
   and returned to the dtx user.
 - *STABLE* sufficient number of sent [[*operation][operations]] have been "persisted"
   on the remote end which guarantees survivial of persistent
   failures.
 - *DONE* all sent [[*operation][operations]] have been "persisted" on sufficient
   number of non-failed participants.

** DTM0 messages
For all scenarios DTM0 protocol identifies the following set of messages:
 - *txr* message is being used to send operations which have to be
   executed on the participants.
 - *EXECUTED* message is being sent from the given participant to the
   originator of dtx, can be a part of *txr* reply message, it's being
   sent when the operation has been executed inside the participant's
   memory and result of the operation is known (e.g. key exists in the
   btree, etc.).
 - *PERSISTENT* message is being sent from the given participant with
   persistent storage to all other participants of dtx when the
   operation effects get persisted (synced to the underlying storage)
   on the given participant.
 - *REDO* message is being used to resend *txr* messages during DTM0
   recovery and being sent from ONLINE participants of the dtx to
   RECOVERING.

** HA messages
Besides existing HA messages like entry point requests/replies, DTM0
algorithm assumes that the following will be introduced and
implemented in HA:
- *participant state change message*, is being sent from HA to participants
  (example: participant_1 goes to RECOVERING state)
- *participant is ready for the recovery*, is being sent from the
  participant to HA when it's ready to accept REDO messages from other
  participants (example: participant gets connected to others).
- *recovery DONE*, is being sent from the participant to HA when it
  has completed the recovery or recovery has failed for any reason.

** Persistent state changes
Different persistent strucutres updates including and not excluding
local BE transactional updates to DTM0 log and CAS service indexes.

* Logical specification
#+BEGIN_QUOTE
=[This section defines a logical structure of the designed component: the decomposition showing how the functional specification is met. Subcomponents and diagrams of their interrelations should go in this section]=
#+END_QUOTE

This section describes "moving parts" of motr services and motr apps
affected by DTM0 feature along with new components needed to satisfy
requirements and functional specification, including their properties
and implementation details. A very high-level message flow is
presented for happy and recovery path. Detailed usecases, state
analysis and intercomponent dependencies are presetend in
corresponding sections.

** Components and their properties
#+NAME: fig:func-spec
[[./logical-spec.svg]]

The following describes a list of components and expected changes
which are in scope of DTM0 design:
*** Clovis interface
There're two options to integrate [[* dtx interface][dtx interface]]. Both have their pros and cons.
 - Propagate dtx as a parameter of clovis interface. In this case user
   has to rely on dtx state machine and wait for corresponding state
   changes.
 - Leave interface as is and hide dtx interface usage inside it. Pros:
   a bit more easier integration. Cons: Will delay "DTM0+" interface
   integration.
*** DTX state machine
Described in [[* dtx interface][dtx interface]] and [[State][state]] sections.
*** Version generator
version generation is based on physical clock and has to be a process/participant-wide unique value.
*** DTX id generator
For each distributed transaction it's needed to generate a
cluster-wide unique identifier.  It was decided to base this
identifier on top of concatenation of version generator and participant
identifier as a "lower nibble".
*** DIX client
DIX client request state machines may be changed w.r.t. dtx states
handling and w.r.t. removal of not useful states used before
(DEL_PHASE2). dtx state trasition may rely on return codes comming
from dix/cas request processing framework.
*** CAS service/FOM
 - dtx part has to be executed in CAS FOM trasactional context.
 - dtx part credits have to be calculated in corresponding CAS FOM callback.
*** DTM0 service/FOM
Additional FOM which is needed for processing REDO, PERSISTENT DTM0
messages in the context of a local transaction. Updates DTM log and persistent structures.
*** HA messages
Described in corresponding section of [[Functional specification][functional specification]].
*** DTM0 messages
Described in corresponding section of [[Functional specification][functional specification]].
*** DTM0 log
Persistent structure storing partial states of execution of dtx seen
on participant's side. Format of the log has to be defined in "DLD of
DTM0 log", still basically this structure provides a store and
iteraction interface over log records. Each log record contains at least the following information:
 - dtx id
 - payload (PUT/DEL key, value, version)
 - list of participants
 - dtx state seen on participants.

*** Persistent structures (btree)
Structure which is being used to store persistent effects introduced by the operation.
*** Persistent structures concurrency, versions, tombstones
Concurrency mechanism used to order conflicting incoming
operations. Tombstones mark deleted records and garbage collected with
a special algorithm based on version comparison.

** Basic flow of happy path
Possible implementation of messages callbacks can be seen in an appropriate section.
#+begin_src plantuml :file happy.png :exports results
originator -> p1: txr
originator -> p2: txr
originator -> p3: txr

p1 -> originator: EXECUTED
p2 -> originator: EXECUTED
p3 -> originator: EXECUTED

activate p1
p1 -> p2         : PERSISTENT
p1 -> p3	 : PERSISTENT
p1 -> originator : PERSISTENT
deactivate p1

activate p2
p2 -> p1         : PERSISTENT
p2 -> p3	 : PERSISTENT
p2 -> originator : PERSISTENT
deactivate p2

activate p3
p3 -> p2         : PERSISTENT
p3 -> p1	 : PERSISTENT
p3 -> originator : PERSISTENT
deactivate p3

#+end_src

#+RESULTS:
[[file:happy.png]]

** Basic flow of recovery path
 - HARE notifies participants about process state changes (messages
   are not shown).
 - ONLINE participants send REDO messages on recovering participant.
 - Recovering participant executes REDO messages if needed.
 - REDO messages can arrive out of order.

#+begin_src plantuml :file recovery.png :exports results
originator -> p1: txr
originator -> p2: txr
originator -> p3: txr

p2 -> originator: EXECUTED
p3 -> originator: EXECUTED
destroy p1
...some time latter, p1 restarted by HARE...

p2 -> p1	 : REDO
p1 -> originator : EXECUTED
p3 -> p1	 : REDO
originator -> p1 : REDO
#+end_src

#+RESULTS:
[[file:recovery.png]]

** Basic callbacks
 - on txr receipt
#+BEGIN_SRC
if (!already_executed(txr)) {
	tx_open(be_tx);
	result = execute(be_tx, txr);
	txr.state[self] = EXECUTED;
	log(be_tx, txr);
	tx_close();
}
sender.send(EXECUTED, txr.id, result);
#+END_SRC

 - on commit (local transaction containing txr is logged):
#+BEGIN_SRC
for (process in txr.participants) { /* including self */
	/* send with retries until reply or receiver failure. */
	process.send(PERSISTENT, txr.id, self);
}
#+END_SRC

 - on PERSISTENT(txid, process) receipt:
#+BEGIN_SRC
txr = log.find(txid);
tx_open(be_tx);
txr.state[process] = PERSISTENT;
/* +1 to account for txr.state[self] */
if (count(txr.state[], state == PERSISTENT) + 1 > K) {
	wakeup(tx); /* tx is STABLE */
} if (all(txr.state[], state == PERSISTENT || proc.state == FAILED)) {
	log.prune(txid); /* tx is DONE */
}
tx_close(be_tx);
#+END_SRC

- on HA.state(process, ONLINE) receipt:
#+BEGIN_SRC
for (txr in log) {
	if (process in txr.participants[] && txr.state[process] < PERSISTENT) {
		process.send(REDO, txr);
	}
}
#+END_SRC

- on HA.state(process, FAILED) receipt:
#+BEGIN_SRC
for (txr in log) {
	if (all(txr.state[], state == PERSISTENT || process.state == FAILED)) {
		log.prune(txid); /* tx is DONE */
	}
}
#+END_SRC

** Conformance
=[For every requirement in the Requirements section, this sub-section explicitly describes how the requirement is discharged by the design. This section is part of a requirements tracking mechanism, so it should be formatted in some way suitable for (semi-)automatic processing]=
** Dependencies
#+BEGIN_QUOTE
=[This sub-section enumerates other system and external components the component depends on. For every dependency a type of the dependency (uses, generalizes, etc.) must be specified together with the particular properties (requirements, invariants) the design depends upon. This section is part of a requirements tracking mechanism]=
#+END_QUOTE

Obvious dependencies on DTM are clovis applications like S3Servers, m0crates, etc.
Obvious DTM0 dependencies are related to HA messages described above.
** Refinement
=[This sub-section enumerates design level requirements introduced by the design. These requirements are used as input requirements for the detailed level design of the component. This sub-section is part of a requirements tracking mechanism]=
* State
#+BEGIN_QUOTE
=[This section describes the additions or modifications to the system state (persistent, volatile) introduced by the component. As much of component behavior from the logical specification should be described as state machines as possible. The following sub-sections are repeated for every state machine]=
#+END_QUOTE

** dtx state machine and triggers
#+begin_src plantuml :file txr.png :exports results
[*] --> EXECUTED	 : N executed messages received.
EXECUTED --> PERSISTENT  : N persistent messages received.
PERSISTENT --> STABLE    : N+K persistent messages received.
STABLE --> DONE
DONE --> [*]

EXECUTED --> FAILED      : HA reports N participants failures
PERSISTENT --> FAILED    : HA reports N participants failures
STABLE --> [*]           : HA reports N participants failures
FAILED --> [*]
#+end_src

#+RESULTS:
[[file:txr.png]]

** participants state machine controlled by HA and expected on motr side
#+begin_src plantuml :file ha.png :exports results
[*] --> RECOVERING
RECOVERING --> ONLINE    : DTM0 log is fully replayed
ONLINE --> TRANSIENT     : HA reported\nthe participant's transient failure
TRANSIENT --> RECOVERING : HA reported\nthe participant restarted
RECOVERING --> ONLINE    : DTM0 recovery completed,\nDTM0 log fully replayed

ONLINE --> PERMANENT     : HA reported\nthe participant's permanent failure
TRANSIENT --> PERMANENT  : HA reported\nthe participant's permanent failure

PERMANENT --> [*]        : not in the scope of HLD
#+end_src

#+RESULTS:
[[file:ha.png]]

** States, events, transitions
=[This sub-section enumerates state machine states, input and output events and state transitions incurred by the events with a table or diagram of possible state transitions. UML state diagrams can be used here]=
** State invariants
=[This sub-section describes relations between parts of the state invariant through the state modifications]=
** Concurrency control
=[This sub-section describes what forms of concurrent access are possible and what forms on concurrency control (locking, queuing, etc.) are used to maintain consistency]=
* Use cases
=[This section describes how the component interacts with rest of the system and with the outside world]=
** Scenarios
=[This sub-section enumerates important use cases (to be later used as seed scenarios for ARID) and describes them in terms of logical specification]=
*** Happy path
#+begin_src plantuml :file happy-path.png :exports results
' The diagram describes a use-case where
' the participants did not experience
' a failure (the so-called "happy-path").
' The goal of the diagram is to show the interactions
' between CAS client/service and DTM library/service.

header DTM Happy-Path \n By Ivan A. 2020-14-12.
footer Page %page% of %lastpage%

title DTM Happy-Path

' The types of processes used here:
' 	- Originators (participants without peristent storage)
' 	- Participants (participants with persistent storage).


box "Client (0)"
	participant "Motr Client" as user
	participant "DIX0" as DIX0
	participant "DTM0" as DTM0
end box

box "ServerLeft (1)"
	participant "CAS1" as CAS1
	participant "DTM1" as DTM1
end box

box "ServerMiddle (2)"
	participant "CAS2" as CAS2
	participant "DTM2" as DTM2
end box

box "ServerRight (3)"
	participant "CAS3" as CAS3
	participant "DTM3" as DTM3
end box

' Prepare op and start execution on (1) 2{{{

activate user
user -> DIX0: clnt_op = { op=PUT, records=k1-k16,v1-v6 }
DIX0 -> CAS1: GET_LAYOUT_AND_PVER
CAS1 -> DIX0: layout_info (pool version, layout id, etc.)
DIX0 -> DIX0: layout = clnt_op.records.map(m0_dix_target_(layout_info, record))
DIX0 -> DTM0: dtx_open(...) returns dtx { clock: now(), tx_id: new_id() }
DIX0 -> DTM0: dtx.add_and_close(pa_p=layout, pa_v=DIX0.fid)
DTM0 -> DTM0: dtx = { clock, tx_id, \npa_v=self.fid,\n pa_p=layout,\n states[1:3]=[FUTURE] }
DIX0 -> DIX0: dix_req[1:3] = { dtx, layout, records, src: DIX0.fid, dst: CAS[i].fid }
DIX0 -> DTM0: dtx_prepared(dtx, dix_req[])
DTM0 -> DTM0: dtx.states = [PREPARED] * 3
DTM0 -> DIX0: ok, ready
DIX0 -> CAS1: send(dix_req[1]) "REQUEST"
DIX0 -> DTM0: dtx_inprogress(dtx, dix_req[1])

' 2}}}

' Execute it on (1) 2{{{
activate CAS1
DIX0 -> CAS2: send(dix_req[2])
DIX0 -> DTM0: dtx_inprogress(dtx, dix_req[2])
activate CAS2
DIX0 -> CAS3: send(dix_req[3])
DIX0 -> DTM0: dtx_inprogress(dtx, dix_req[3])
activate CAS3
CAS1 -> DTM1: dtx_recv(fom.fop.request.dtx)
DTM1 -> CAS1: returns (dtx, ready)
CAS1 -> CAS1: FOM_TX_OPEN: fom.ltx = be_tx.open()
CAS1 -> CAS1: FOM_CAS_LOOP: cas_reply = cas_exec(fom)
CAS1 -> CAS1: FOM_SUCCESS: fom.fop.reply = cas_reply
CAS1 -> DTM1: FOM_FOL_REC_ADD: dtm_log_add(fom)
CAS1 -> CAS1: FOM_TX_COMMIT: fom.ltx.close()
CAS1 -> CAS1: FOM_QUEUE_REPLY: rpc_post(fom.fop.reply)
CAS1 -> DIX0: send reply "EXECUTED" {dtx, cas_reply}
deactivate CAS1
DIX0 -> DTM0: dtx_executed(reply.dtx)
DIX0 -> user: lookup_op(reply.dtx).on_executed()
deactivate user
activate CAS1
' }}}

' CAS1 persistent 2{{{
CAS1 -> DTM1: dtm_log_peristent(dtx, [pa=self])
activate DTM1
DTM1 -> DTM0: send(PERSISTENT, dtx)
DTM0 -> DIX0: on_persistent(dtx)
DIX0 -> user: lookup_op(dtx).on_persistent()
activate user
DTM1 -> DTM2: send(PERSISTENT, dtx)
DTM1 -> DTM3: send(PERSISTENT, dtx)
deactivate CAS1
deactivate DTM1
DTM0 -> DTM0: dtm_log_persistent(dtx, pa=CAS1)
DTM2 -> DTM2: dtm_log_persistent(dtx, pa=CAS1)
DTM3 -> DTM3: dtm_log_persistent(dtx, pa=CAS1)
' }}}

' CAS2 execution 2{{{
CAS2 -> DTM2: dtx_recv
CAS2 -> CAS2: execute ...
CAS2 -> DTM2: dtm_log_add(fom)
CAS2 -> DIX0: send reply "EXECUTED" {dtx, cas_reply}
' 2}}}
'
' CAS2 persistent 2{{{
CAS2 -> DTM2: dtm_log_persistent(dtx, [pa=self])
DTM2 -> DTM0: send(PERSISTENT, dtx)
DTM2 -> DTM1: send(PERSISTENT, dtx)
DTM2 -> DTM3: send(PERSISTENT, dtx)
' 2}}}

' CAS2 mark persistent 2{{{
DTM0 -> DTM0: dtm_log_persistent(dtx, pa=CAS2)
DTM1 -> DTM1: dtm_log_persistent(dtx, pa=CAS2)
DTM3 -> DTM3: dtm_log_persistent(dtx, pa=CAS2)
' 2}}}

' CAS3 execution 2{{{
CAS3 -> DTM3: dtx_recv
CAS3 -> CAS3: execute ...
CAS3 -> DTM3: dtm_log_add(fom)
CAS3 -> DIX0: send reply "EXECUTED" {dtx, cas_reply}
' 2}}}

' CAS3 persistent 2{{{
CAS3 -> DTM3: dtm_log_persistent(dtx, [pa=self])
DTM3 -> DTM0: send(PERSISTENT, dtx)
DTM3 -> DTM1: send(PERSISTENT, dtx)
DTM3 -> DTM2: send(PERSISTENT, dtx)
DTM3 -> DTM3: dtm_log_stable(dtx)
' 2}}}

' CAS3 mark persistent 2{{{
DTM0 -> DTM0: dtm_log_persistent(dtx, pa=CAS3)
DTM0 -> DTM0: dtm_log_stable(dtx)
DTM0 -> DIX0: on_stable(dtx)
DIX0 -> user: lookup_op(dtx).on_stable()
DTM1 -> DTM1: dtm_log_persistent(dtx, pa=CAS3)
DTM1 -> DTM1: dtm_stable(dtx)
DTM2 -> DTM2: dtm_log_persistent(dtx, pa=CAS3)
DTM2 -> DTM2: dtm_stable(dtx)
' 2}}}
#+end_src

#+RESULTS:
[[file:happy-path.png]]

*** 1 transient failure of persistent participant and subsequent DTM recovery
#+BEGIN_SRC
# Use-case description

- 3 participants with persistent storage (p1, p2, p3);
- 1 participant (originator) without persistent storage;
- S3 client wants to create 3 objects with metadata;
- S3 server and a part of the cluster gets rebooted;
- S3 client time-out and sends a request to delete the objects.
- DTM ensures consistency of data across the cluster.

# Events and states

Legend:
	o1, p1, p2, p3 - file operation logs (persistent and volatile parts).
	I,V,P - state of participants of a log entry (e.g, st=VVV means "assume it to be executed everywhere if no one failed").
		I - InProgress (to-be-sent/to-be-executed);
		V - Volatile (Executed);
		P - Persistent (got-the-notice/got-comitted).
	NETWORK,DISK,PROC - events in the system (RPC messages, HA messages, fsync notifications, process states).

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
NETWORK,DISK,PROC	User				o1				p1				p2				p3
|			|				|				|				|				|
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
@10-15:
			{ [DIX PUT k1 @10] }
			{ [DIX PUT k2 @11] }
			{ [DIX PUT k3 @12] }

						{
						R1[CAS PUT k1 @10 st=III],
						R2[CAS PUT k2 @11 st=III],
						R3[CAS PUT k3 @12 st=III],
						}

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
@20:
o1 -> p1: request o1.R1, request o1.R2, request o1.R3
o1 -> p2: request o1.R1
o1 -> p3: request o1.R1, request o1.R2, request o1.R3
# The requests to p2 with o1.R1 and o1.R3 got stuck
#  somewhere (for example, the RPC messages were not yet scheduled to be sent out).

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
@25:
p1: received o1.R1, o1.R2, o1.R3
p2: received o1.R1
p3: received o1.R1, O1.R2, o1.R3
# Due to network re-ordering (or re-ordering in some request handler queue),
# the request arrive in different order.
						{					{				{				{
						R1[CAS PUT k1 @10 st=III],		R1[CAS PUT k1 @10 st=III],	R1[CAS PUT k2 @11 st=III],	R1[CAS PUT k3 @12 st=III],
						R2[CAS PUT k2 @11 st=III],		R2[CAS PUT k2 @11 st=III],					R2[CAS PUT k2 @11 st=III],
						R3[CAS PUT k3 @12 st=III],		R3[CAS PUT k3 @12 st=III],					R3[CAS PUT k1 @10 st=III],
						}					}				}				}

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
@30-40:
# The request are executed
						{					{				{				{
						R1[CAS PUT k1 @10 st=III],		R1[CAS PUT k1 @10 st=VVV],	R1[CAS PUT k2 @11 st=VVV],	R1[CAS PUT k3 @12 st=VVV],
						R2[CAS PUT k2 @11 st=III],		R2[CAS PUT k2 @11 st=III],					R2[CAS PUT k2 @11 st=III],
						R3[CAS PUT k3 @12 st=III],		R3[CAS PUT k3 @12 st=III],					R3[CAS PUT k1 @10 st=III],
						}					}				}				}

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
@50:
p1 -> o1: reply p1.R1
p2 -> o1: reply p1.R1
p3 -> o1: reply p1.R1
# Reply was sent for each executed request.
						{					{				{				{
						R1[CAS PUT k1 @10 st=VII],		R1[CAS PUT k1 @10 st=VVV],	R1[CAS PUT k2 @11 st=VVV],	R1[CAS PUT k3 @12 st=VVV],
						R2[CAS PUT k2 @11 st=IVI],		R2[CAS PUT k2 @11 st=III],					R2[CAS PUT k2 @11 st=III],
						R3[CAS PUT k3 @12 st=IIV],		R3[CAS PUT k3 @12 st=III],					R3[CAS PUT k1 @10 st=III],
						}					}				}				}


---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
@60:
p1disk -> p1: committed(R1)
p2disk -> p2: committed(R1)
p3disk -> p3: committed(R1)
# As you see we have 3 different records to reach persistent states on 3 different nodes:
						{					{				{				{
						R1[CAS PUT k1 @10 st=VVV],		R1[CAS PUT k1 @10 st=PVV],	R1[CAS PUT k2 @11 st=VPV],	R1[CAS PUT k3 @12 st=VVP],
						R2[CAS PUT k2 @11 st=VVV],		R2[CAS PUT k2 @11 st=VVV],					R2[CAS PUT k2 @11 st=VVV],
						R3[CAS PUT k3 @12 st=VVV],		R3[CAS PUT k3 @12 st=VVV],					R3[CAS PUT k1 @10 st=VVV],
						}					}				}				}


---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
@70:
p1proc: reboot
p3proc: reboot
o1proc: reboot as o2
# Assume: o1proc co-exists with p1proc

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
@80-85:
HA -> p1: CRASHED
HA -> p3: CRASHED
HA -> o2: INIT (?)
p1 -> HA: ready to RECOVERING@80
p3 -> HA: ready to RECOVERING@81
o2 -> HA: ready to ONLINE@82

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
@90:
HA -> p2: p1 is RECOVERING@80
HA -> p2: p3 is RECOVERING@81
HA -> p2: o1 is OFFLINE@80
HA -> p2: o2 is ONLINE@82

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
WARNING: The section below is not finished yet :WARNING
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
NETWORK,DISK,PROC	User				o2				p1				p2				p3
|			|				|				|				|				|
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
@100-105:
# S3 client sends a DEL operation to remove the objects.
# It leads to 3 calls to motr client which:
			{ [DIX DEL k1] }
			{ [DIX DEL k2] }
			{ [DIX DEL k3] }
						{					{				{				{
						R1[CAS DEL k1 @100 st=III],		R1[CAS PUT k1 @10 st=PVV],	R1[CAS PUT k2 @11 st=VPV],	R1[CAS PUT k3 @12 st=VVP],
						R2[CAS DEL k2 @101 st=III],
						R3[CAS DEL k3 @102 st=III],
						}					}				}				}

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
@110:
o2 -> p1: request o2.R1, request o2.R2, request o2.R3
o2 -> p2: request o2.R1, request o2.R2, request o2.R3
o2 -> p3: request o2.R1, request o2.R2, request o2.R3

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
@115:
p1: received o2.R1, received o2.R2, received o2.R3
p2: received o2.R1, received o2.R2, received o2.R3
p3: received o2.R1, received o2.R2, received o2.R3

						{					{				{				{
						R1[CAS DEL k1 @100 st=III],		R1[CAS PUT k1 @10  st=PVV],	R1[CAS PUT k2 @11  st=VPV],	R1[CAS PUT k3 @12  st=VVP],
						R2[CAS DEL k2 @101 st=III],		R2[CAS DEL k1 @100 st=III],	R2[CAS DEL k1 @100 st=III],	R2[CAS DEL k1 @100 st=III],
						R3[CAS DEL k3 @102 st=III],		R3[CAS DEL k2 @101 st=III],	R3[CAS DEL k2 @101 st=III],	R3[CAS DEL k2 @101 st=III],
											R4[CAS DEL k3 @102 st=III],	R4[CAS DEL k3 @102 st=III],	R4[CAS DEL k3 @102 st=III],
						}					}				}				}

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
@120:
## Here are the messages that have to be sent:
# Recovering of o1 on p1
p1 -> p2: REDO p1.R1
p1 -> p3: REDO p1.R1
# Recovering of p1
p2 -> p1: REDO p2.R1
# Recovering of o1 on p2
p2 -> p1: REDO p2.R1
p2 -> p3: REDO p2.R1
# Recovering of p3
p2 -> p3: REDO p2.R1
# Recovering of o1 on p3
p3 -> p1: REDO p3.R1
p3 -> p2: REDO p3.R1
## Here are the message that have to actually be sent (without duplicates):
p1 -> p2: REDO p1.R1
p1 -> p3: REDO p1.R1
p2 -> p1: REDO p2.R1
p2 -> p3: REDO p2.R1
p3 -> p1: REDO p3.R1
p3 -> p2: REDO p3.R1
# Note: duplicates are ommitted in the FOLs.

						{					{				{				{
						R1[CAS DEL k1 @100 st=III],		R1[CAS PUT k1 @10  st=PVV],	R1[CAS PUT k2 @11  st=VPV],	R1[CAS PUT k3 @12  st=VVP],
						R2[CAS DEL k2 @101 st=III],		R2[CAS DEL k1 @100 st=III],	R2[CAS DEL k1 @100 st=III],	R2[CAS DEL k1 @100 st=III],
						R3[CAS DEL k3 @102 st=III],		R3[CAS DEL k2 @101 st=III],	R3[CAS DEL k2 @101 st=III],	R3[CAS DEL k2 @101 st=III],
											R4[CAS DEL k3 @102 st=III],	R4[CAS DEL k3 @102 st=III],	R4[CAS DEL k3 @102 st=III],
											R5[CAS PUT k2 @11  st=III],	R5[CAS PUT k1 @10  st=III],	R5[CAS PUT k1 @10  st=III],
											R6[CAS PUT k3 @12  st=III],	R6[CAS PUT k3 @12  st=III],	R6[CAS PUT k2 @11  st=III],
						}					}				}				}

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
@130:
# The requests are executed. The KVS on each node will have the following content if
# p1 does not put tombstones:

p1: kvs = <empty set>, tombstones = { k1@100, k2@101, k3@102 }
p2: kvs = { k1@10, k3@12 }, tombstones = <empty set>
p3: kvs = <empty set>, tombstones = { k1@100, k2@101, k3@102 }

# However, if p1 was able to put tombstones then all the KVS would have had <empty set>,
# which would be the expected behavior in this situation.
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
#+END_SRC
*** 1 transient failure of persistent participant and subsequent another 1 subsequent failure during recovery
#+begin_src plantuml :file one-transient.png :exports results
' The diagram describes a use-case where
' the system experienced transient
' a failure. One of the nodes lost its volatile
' state, and leads to the DTM "recovering" procedures.
' The goal of the diagram is to show the interactions
' between CAS client/service and DTM library/service
' and the HA.

header DTM One Transient \n By Ivan A. 2020-14-12.
footer Page %page% of %lastpage%

title DTM One Transient

' The types of processes used here:
' 	- Originators (participants without peristent storage)
' 	- Participants (participants with persistent storage).

box "Client (0)" #00ffcc
	participant "Motr Client" as user
	participant "DIX0" as DIX0
	participant "DTM0" as DTM0
	participant "HA0" as HA0
end box

box "ServerLeft (1)" #b3daff
	participant "CAS1" as CAS1
	participant "DTM1" as DTM1
	participant "HA1" as HA1
end box

box "ServerMiddle (2)"   #66b5ff
	participant "CAS2" as CAS2
	participant "DTM2" as DTM2
	participant "HA2" as HA2
end box

box "ServerRight (3)"   #339cff
	participant "CAS3" as CAS3
	participant "DTM3" as DTM3
	participant "HA3" as HA3
end box

' Prepare op and start execution on (1) 2{{{

user -> DIX0: clnt_op = { op=PUT, records=k1-k16,v1-v6 }
DIX0 -> CAS1: GET_LAYOUT_AND_PVER
CAS1 -> DIX0: layout_info (pool version, layout id, etc.)
DIX0 -> DIX0: layout = clnt_op.records.map(m0_dix_target_(layout_info, record))
DIX0 -> DTM0: dtx_open(...) returns dtx { clock: now(), tx_id: new_id() }
DIX0 -> DTM0: dtx.add_and_close(pa_p=layout, pa_v=DIX0.fid)
DTM0 -> DTM0: dtx = { clock, tx_id, \npa_v=self.fid,\n pa_p=layout,\n states[1:3]=[FUTURE] }
DIX0 -> DIX0: dix_req[1:3] = { dtx, layout, records, src: DIX0.fid, dst: CAS[i].fid }
DIX0 -> DTM0: dtx_prepared(dtx, dix_req[])
DTM0 -> DTM0: dtx.states = [PREPARED] * 3
DTM0 -> DIX0: ok, ready
' 2}}}

group Sending requests
	DIX0 -> CAS1: send(dix_req[1])
	DIX0 -> DTM0: dtx_inprogress(dtx, dix_req[1])
	DIX0 -> CAS2: send(dix_req[2])
	DIX0 -> DTM0: dtx_inprogress(dtx, dix_req[2])
	DIX0 -> CAS3: send(dix_req[3])
	DIX0 -> DTM0: dtx_inprogress(dtx, dix_req[3])
end

group CAS1 execution
	CAS1 -> DTM1: dtx_recv(fom.fop.request.dtx)
	DTM1 -> CAS1: returns (dtx, ready)
	CAS1 -> CAS1: FOM_TX_OPEN: fom.ltx = be_tx.open()
	CAS1 -> CAS1: FOM_CAS_LOOP: cas_reply = cas_exec(fom)
	CAS1 -> CAS1: FOM_SUCCESS: fom.fop.reply = cas_reply
	CAS1 -> DTM1: FOM_FOL_REC_ADD: dtm_log_add(fom)
	CAS1 -> CAS1: FOM_TX_COMMIT: fom.ltx.close()
	CAS1 -> CAS1: FOM_QUEUE_REPLY: rpc_post(fom.fop.reply)
	CAS1 -> DIX0: send reply "EXECUTED" {dtx, cas_reply}
	DIX0 -> DTM0: dtx_executed(reply.dtx)
	DIX0 -> user: lookup_op(reply.dtx).on_executed()
end

=== Server (1) powered off ===

group  Server (1) got crached
	CAS1 ->X HA1: No heartbeats
	DTM1 ->X HA1: No heartbeats
	CAS1 -> CAS1: Lost volatile state
	DTM1 -> DTM1: Lost volatile state
end

group CAS2 execution
	CAS2 -> DTM2: dtx_recv
	CAS2 -> CAS2: execute ...
	CAS2 -> DTM2: dtm_log_add(fom)
	CAS2 -> DIX0: send reply "EXECUTED" {dtx, cas_reply}
end

group CAS2 persistent
	CAS2 -> DTM2: dtm_log_persistent(dtx, [pa=self])
	DTM2 -> DTM0: send(PERSISTENT, dtx)
	DTM0 -> DIX0: on_persistent(dtx)
	DIX0 -> user: lookup_op(dtx).on_persistent()
	DTM2 ->X DTM1: send(PERSISTENT, dtx)
	DTM2 -> DTM3: send(PERSISTENT, dtx)
end

group CAS3 execution
	CAS3 -> DTM3: dtx_recv
	CAS3 -> CAS3: execute ...
	CAS3 -> DTM3: dtm_log_add(fom)
	CAS3 -> DIX0: send reply "EXECUTED" {dtx, cas_reply}
end

group CAS3 persistent
	CAS3 -> DTM3: dtm_log_persistent(dtx, [pa=self])
	DTM3 -> DTM0: send(PERSISTENT, dtx)
	DTM3 ->X DTM1: send(PERSISTENT, dtx)
	DTM3 -> DTM2: send(PERSISTENT, dtx)
end

=== Server (1) powered on ===

group Server(1) enters "recovering" of records until E
	CAS1 -> DTM1: Ready for DTM redo @E
	note right: @E is the point where CAS1 \nis ready to receive requests
	DTM1 -> HA1: Ready for DTM redo @E
	HA1 -> HA0: Server(1) is ready for redo @E
	HA1 -> HA2: Server(1) is ready for redo @E
	HA1 -> HA3: Server(1) is ready for redo @E
end

group REDO from Server(2)
	HA2 -> DTM2: REDO Server(1)@E
	loop forall dtx in log until dtx.clock < @E
		DTM2 -> DTM1: send(REDO, dtx, seqid, max_seqid)
		DTM1 -> DTM1: dtx_recv
		DTM1 -> CAS1: execute ...
		CAS1 -> DTM1: dtm_log_add(fom, persistent=dtx.state)
	end
	DTM1 -> HA1: done REDO from Server(2)@E
end

group REDO from Client (0)
	HA0 -> DTM0: REDO Server(1)@E
	loop forall dtx in log until dtx.clock < @E
		DTM0 -> DTM1: send(REDO, dtx, seqid, max_seqid)
		DTM1 -> DTM1: dtx_recv(dtx, ...)
		DTM1 -> DTM1: dtm_log_lookup(dtx.tx_id) -> record\ndup detected
		DTM1 -> CAS1: cas_reply = record.reply
		CAS1 -> DIX0: send reply "EXECUTED" {dtx, cas_reply}
		CAS1 -> DTM1: dtm_log_add(fom, persistent=dtx.state)
	end
	DTM1 -> HA1: done REDO from Client(0)@E
end

group REDO from Server(3)
	HA2 -> DTM3: REDO Server(1)@E
	loop forall dtx in log until dtx.clock < @E
		DTM3 -> DTM1: send(REDO, dtx, seqid, max_seqid)
		DTM1 -> DTM1: dtm_log_lookup(dtx.tx_id) -> record\ndup detected
		DTM1 -> DTM1: dtm_log_add(fom, persistent=dtx.state)
	end
	DTM1 -> HA1: done REDO from Server(1)@E
end

group Presistent notices from the recovering server.\nThey can be sent at any moment\nduring recovery.
	' Self
	CAS1 -> DTM1: dtm_log_persistent(dtx, [pa=self])
	DTM1 -> DTM1: on_stable(dtx)
	' Client(0)
	DTM1 -> DTM0: send(PERSISTENT, dtx)
	DTM0 -> DTM0: dtm_log_persistent(dtx, pa=CAS1)
	DTM0 -> DTM0: dtm_log_stable(dtx)
	DTM0 -> DIX0: on_stable(dtx)
	DIX0 -> user: lookup_op(dtx).on_stable()
	' Server(2)
	DTM1 -> DTM2: send(PERSISTENT, dtx)
	DTM2 -> DTM2: dtm_log_persistent(dtx, pa=CAS1)
	DTM2 -> DTM2: dtm_log_stable(dtx)
	' Server(3)
	DTM1 -> DTM3: send(PERSISTENT, dtx)
	DTM3 -> DTM3: dtm_log_persistent(dtx, pa=CAS1)
	DTM3 -> DTM3: dtm_log_stable(dtx)
end

=== Server (1) enters ONLINE ===

group HA notifies about Online state for Server(1)
	HA1 -> HA0: Server(1) is Online
	HA1 -> HA2: Server(1) is Online
	HA1 -> HA3: Server(1) is Online
end
#+end_src
#+RESULTS:
[[file:one-transient.png]]

*** N transient failures of originators
*** N transient failures of originators and k failures of persistent participants
*** DTM recovery stop
#+BEGIN_SRC

/*
Allowed transitions:
        CRASHED -----------> RECOVERING -------------> ONLINE;
	/|\                                              |
         +-----------------------------------------------+

Timelines and periods:

st=ONLINE     st=CRASHED             st=RECOVERING          st=ONLINE
|-------X-----|----------------------|----------------------|--------> t
        |        ("boot" period)         ("grace" period)     ("normal" mode)
        |
     (actual crash)

*/


// Types -- verified by invariants or a type system (if there any).
struct redo_state_item {
	fid  node;
	bool is_done;
};

// In-memory states (state machines)
variable: redo_state, state space: any possible value of redo_state_item[N],
	where N depends on @T and it is in the range 0..NR_NODES.
variable: HA; // opaque object that generates HA events
variable: NETWORK; // opaque object that sends/receives REDO messages
variable: DTM; // opaque object that wraps around DTM log and CAS service

/* TODO: redo_state has to be a part of separate array of variables
	that represent volatile state of each participant
*/

// Transistions of in-memory states

// This function affects redo_state variable
redo_state.on_ha_event(RECOVERING self) {
	assume: HA.prev_state() == CRASHED;

	redo_state = HA.get_online_dones().map(\n -> (n.fid, false));
}

(DTM|HETWORK).redo_fom_ver1(!ONLINE other){
	// Find the last record to send
	prev_record = NULL
	last_record = NULL;
	for record in DTM.log {
		// Some magic with prev and current records
		// .........
		// may be omitted here.
		if record.time > HA.recovery_start_time(other) {
			last_record = prev_record;
			break;
		}
	}

	msg.src = self;
	msg.tgt = other;

	for record in DTM.log {
		if (!record.has_pa(other) ||
		    record.has_persistent_msg(other)) {
			continue;
		}
		// Can use batching of records here
		msg.dtx = record.dtx;
		msg.is_last = record == last_record;
		NETWORK.send(msg);
		await NETWORK.recv(ack);
		// Execution is synchrnonous but
		// we still have to match acks.
		assume: ack.dtx.tx_id == msg.dtx.tx_id.
		if (record == last_record)
			break;
	}
}

(DTM|HETWORK).redo_fom_ver2(!ONLINE other){
	msg.src = self;
	msg.tgt = other;
	for record in DTM.log {
		if (record.time > HA.recovery_start_time(other))
			break;

		if (!record.has_pa(other) ||
		    record.has_persistent_msg(other)) {
			continue;
		}

		// Can use batching of records here
		msg.dtx = record.dtx;
		NETWORK.send(msg);
		await NETWORK.recv(ack);
		// Execution is synchrnonous but
		// we still have to match acks.
		assume: ack.dtx.tx_id == msg.dtx.tx_id.
	}
	DTX.redo_complete_msg.src = self;
	DTX.redo_complete_msg.tgt = other;
	NETWORK.send(DTX.redo_complete_msg);
	await NETWORK.recv(ack);
}

// This function affects only NETWORK variable
(NETWORK).on_ha_event(!ONLINE other) {
	assume: self != other;
	if (HA.is_recovering(self)) {
		redo_state.remove(other.fid);
		return;
	}

	if (HA.is_online(self) && HA.is_recovering(other))
		if ((DTM|HETWORK).redo_fom_ver1)
			// With "is_last" field inside of each packet
			enque((DTM|HETWORK).redo_fom_ver1);
		else
			// With the separate completion packet
			enque((DTM|HETWORK).redo_fom_ver2);
}

// This function affects DTM, NETWORK and HA variables
(NETWORK|DTM|HA).on_recv(REDO msg) {
	assume: The message was sent to us.
	assume: Execution is synchronous: acks sent only after a message
	is processed; there is no way to execute the last record
	before all the previous records are executed.
	assume: HA.is_online(msg.src).

	DTM.apply(msg);
	if (msg_with_is_last_field)
		redo_state[msg.src].is_done = msg.is_last;
	ack = { .dtx = msg.dtx, .reply = msg.reply };
	await NETWORK.send(msg); // may include a reply, see DTM.apply

	if (msg_with_is_last_field) {
		if (redo_state[msg.src].is_done)
			redo_state.remove(msg.src);

		if (redo_state.is_empty())
			// The node "self" has been fully recovered.
			HA.consider_st_transition(self, ONLINE, now())
	}
}

(NETWORK|DTM|HA).on_recv(REDO_COMPLETE msg) {
	assume: The message was sent to us.

	redo_state.remove(msg.src);

	if (redo_state.is_empty())
		// The node "self" has been fully recovered.
		HA.consider_st_transition(self, ONLINE, now())

// This function affect only the state of DTM variable
DTM.apply(msg) {
	record = DTM.lookup(msg.tx_id())
	if record.is_none() {
		msg.reply = DTM.CAS.execute(msg);
	} else {
		if msg.src.is_reply_required() {
			msg.reply = record.reply;
		}
	}
	DTM.log_update_state(msg.tx_id(), msg.states());
	async DTM.log_commit(msg.tx_id());
}
#+END_SRC
*** DTM during persistent failures
*** DTM transaction abort (cancelation of clovis operation)
*** Clovis interface w.r.t. distributed transactions
C-level interface
Affected clovis calls
*** HA callbacks
*** DTM Log truncation
*** death of originator
*** Dependency tracking (explicit and inexplicit)
*** DTM message delivery during failures
deliver executed callback after persistent participant failure
*** DTM transaction and record versions
*** clock synchronisation
*** clock desynchronization
*** failure model
*** ismask handling
*** recovery: redo+put/del execution, ordering
*** how long with 3-way replication 3 participant can be in transient?
*** new originator after fail
*** how the states of participants are propagated the dtm logic (confd)
*** startup procedure (recovering startup)
#+begin_src plantuml :file startup.png :exports results
participant originator
participant p1
participant p2
participant p3
participant hax

=== p3 powered on ===
...some time latter, p3 restarted by hare...

hax -> p3: start p3 process
p3 -> hax: entry point request
hax -> p3: entry point reply [conf, profile]

activate p3
p3 -> p2: rpc session establish
p3 -> p1: rpc session establish
p3 -> originator: rpc session establish
deactivate p3

p3 -> hax: p3 is ready to accept new MD/IO traffic

activate hax
hax -> p3: start dtm0 recovery on p3
hax -> p1: ha state, p3 is in recovering state
hax -> p2: ha state, p3 is in recovering state
hax -> originator: ha state, p3 is in recovering state
deactivate hax
...
p2 -> p3: REDO [PUT kj, vj, verj]
p1 -> p3: REDO [PUT kj, vj, verj]
originator -> p3: REDO [PUT kj, vj, verj]
...
p2 -> p3: REDO [PUT kn, vn, vern]
p1 -> p3: REDO [PUT kn, vn, vern]
originator -> p3: REDO [PUT kn, vn, vern]
...
p3 -> hax: recovery DONE
...

hax -> p1: ha state, p1 is in online state
hax -> p2: ha state, p1 is in online state
hax -> p3: ha state, p1 is in online state
hax -> originator: ha state, p1 is in online state


#+end_src

#+RESULTS:
[[file:startup.png]]

*** Cluster shutdown usecase
*** Cluster start usecase after shutdown
*** Metadata update
TBD.
*** S3 interraction
#+BEGIN_SRC
                 failure(transient)
-------------------------------|------------------------------------------------>t

S3req |-------------------------------------------------|
DIX1(w)    |----|
DIX2(w)      |----|
DIX3(w)        |----|
IO1(w)              |-------|
IO2(w)                       |------|
DIX4(w)                              |----|
#+END_SRC
Questions:
0) Assume that IO2 will fail (fig. 1).
1) What S3 server has to do after transient failure happend (fig. 1)?
2) How long the transient failure is suppose to take?
3) Availability implications (9s).
Non-DTM related question:
What to do with persistent failures:
1) (long)  Are we going to have a code in motr which will handle the case of creation of another obj-id during persistent failure?
2) (short) How to handle persisent failures? Describe the scenario.

*** long living tombstones and log records
TBD.
** Failures
=[This sub-section defines relevant failures and reaction to them. Invariants maintained across the failures must be clearly stated. Reaction to Byzantine failures (i.e., failures where a compromised component acts to invalidate system integrity) is described here]=
* Analysis
** Scalability
#+BEGIN_QUOTE
=[This sub-section describes how the component reacts to the variation in input and configuration parameters: number of nodes, threads, requests, locks, utilization of resources (processor cycles, network and storage bandwidth, caches), etc. Configuration and work-load parameters affecting component behavior must be specified here]=
#+END_QUOTE

Possbile implications of DTM0 integration on performance:
 - the number of transactions increased by 3 per each PUT/DEL key value operation.
 - tombstones would require addidional lookup, which can cost a lock time for huge trees.
 - tombstones management would require backgroud GC which would limit number of transactions in flight.

** Other
=[As applicable, this sub-section analyses other aspects of the design, e.g., recoverability of a distributed state consistency, concurrency control issues]=
** Rationale
=[This sub-section describes why particular design was selected; what alternatives (alternative designs and variations of the design) were considered and rejected]=
* Deployment
** Roadmap and execution
#+BEGIN_QUOTE
=[Intercomponent dependencies and execution plan w.r.t. HA and others.]=
#+END_QUOTE

Obviously implementation plan will have HA and S3 related
activities. Possbile schedule of such plan can have the following
micro milestones:
 - DTM0 happy path scenario.
   - Goals:
     - To have a unit- and system- test sending clovis PUT/DEL requests
       using DTM0 logic of including CAS PUT/DEL requests, PERSISENT
       FOP, updating DTM0 log.
   - Features:
     - DTM0 log.
     - DTM0 service and FOPs.
     - DTX state machine embedded into clovis PUT/DEL operations.
     - Integration: A glue around DTM0 service connections; Disable spares.
       Can/Will be dirty implementation.
     - Clock algorithm.
   - Deliverable:
     - Code and demo reviewed by Nikita and management.
     - Recorded review results.

 - DTM0 start/stop scenarios (depends on "HA messages").
   - Goals:
     - Define the scope for HARE and Motr/DTM0 teams related to
       integration of DTM0 and HARE. Produce a spec. Sign the spec by
       architects.
     - To have a system tests covering the following scenarios:
       - basic start with DTM0 services reconnect.
       - follow new HA messages protocol which would include NEW
         recovering message state and recovering complete state
         messages.
       - failure scenarios: transitions from TRANSIENT to RECOVERING
         states by means of service crash and restart.
       - paranoid scenarios: quick reconnect/connect messages from HA,
         duplicated state messages, etc.
   - Deliverable:
     - Code and demo reviewed by Nikita and management.
     - Recorded review results.
 - DTM0 recovery scenarios (depends on "HA messages").
   - Goals:
     - To have a full-fledged DTM0 implementation including recovery
       logic and system tests having a bunch of clients sending
       parallel MD operations to servers.
     - Processed failures: ordinary failures, failures during
       recovery, failures during recovery transitting into PERMANENT
       state.
     - Prepare a set of *debugging tools* to be ready to address
       problems on the real cluster.
 - Basic S3 integration (depends on DTM0).
   - Goals:
     - integrate DTM0 component with S3.
     - participate in test plan defintion.
 - DTM0 test.
   - TBD.
 - DTM0 basic deployment.
   - TBD.
 - DTM0 performance work (not in the scope of DTM0 work).
   - TBD.
 - (future work) Metadata update (depends on basic S3 integration).
   - TBD.

Possible execution plan:
#+BEGIN_SRC
motr: |----------- happy ---|---- recovery scenarios ----|                    |-- dtm0 test --|-- deploy --|
                           /
                          /<---(depends on)
ha  : |-- ha messages --|

s3  :                                                    |-- s3 integration --|
#+END_SRC

** Compatibility
=[Backward and forward compatibility issues are discussed here. Changes in system invariants (event ordering, failure modes, etc.)]=
*** Network
*** Persistent storage
*** Core
=[Interface changes. Changes to shared in-core data structures]=
** Installation
=[How the component is delivered and installed]=
* References
=[References to all external documents (specifications, architecture and requirements documents, etc.) are placed here. The rest of the document cites references from this section. Use Google Docs bookmarks to link to the references from the main text]=
